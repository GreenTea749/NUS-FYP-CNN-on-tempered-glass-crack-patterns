{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreenTea749/NUS-FYP-CNN-on-tempered-glass-crack-patterns/blob/main/glassCNN_shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3KjYFUYwImj"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHW-W9VwwdQv"
      },
      "outputs": [],
      "source": [
        "!python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFKiLrG5wsaV"
      },
      "outputs": [],
      "source": [
        "!nvcc -V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7jOUABKwzcL"
      },
      "source": [
        "Mount the Google drive.\n",
        "\n",
        "Mount your Google drive to store the dataset and the trained models. Execute the cell below. Visit this [URL](https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code) to retrieve the authorization code and enter the code at the prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XXib-Rnwt62"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yeb1s-Aw7AQ"
      },
      "source": [
        "Directory Navigation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3zBz9KJw86E"
      },
      "outputs": [],
      "source": [
        "from os import path, chdir, getcwd, mkdir\n",
        "\n",
        "# Choose a project name\n",
        "projectName = \"Glass-ANN/\"\n",
        "\n",
        "# Project directory is in My Drive\n",
        "projectDirectory = \"/content/drive/My Drive/\" + projectName #content/drive/My Drive/glassCNN/\n",
        "# Checks if cwd is in content folder\n",
        "if getcwd() == \"/content\":\n",
        "  # Makes project directory if it does not exist\n",
        "  if not path.isdir(projectDirectory):\n",
        "    mkdir(projectDirectory)\n",
        "    print(f\"Project {projectName} has been created!\")\n",
        "  else:\n",
        "    print(f\"Project {projectName} already exist!\")\n",
        "  # Changes to project directory\n",
        "  chdir(projectDirectory)\n",
        "\n",
        "print(f\"The current working directory is {getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-s-OZoryt5-"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvNhePMhyvhV"
      },
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJXuzEQ1y-GJ"
      },
      "source": [
        "Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMPHj4Pky_G6"
      },
      "outputs": [],
      "source": [
        "# Check if runtime uses GPU\n",
        "import torch\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "print(\"Using GPU\", gpu_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99jyct0mwPET"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuiaUfehPjoJ"
      },
      "source": [
        "## Dataset with variable number of labels\n",
        "tempered glass dataset:\n",
        "\n",
        "folder:\n",
        "input x (images):\n",
        "    - /all_photos/paper or /all_photos/plastic\n",
        "    - R/P 01.png\n",
        "\n",
        "input y (labels):\n",
        "    - dataset.csv\n",
        "    - 3 parameters,\n",
        "\n",
        "    0: angle\n",
        "    1: velocity\n",
        "    2: kinetic energy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqrzi2VUPMiw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "\n",
        "class LabeledGlassSet(Dataset):\n",
        "    \"\"\"Parameters:\n",
        "            - train_test_defined (bool): Whether to use predefined train/test split.\n",
        "            - is_train (bool): If True, loads training images; otherwise, testing images.\n",
        "            - combine_images (bool): Whether to concatenate corresponding P# and R# images into a single tensor.\n",
        "            - image_type (str): Specifies which images to load when not combining. Options: 'plastic', 'paper', 'both'.\n",
        "            - labels_set (list of int): Indices of the labels to extract from the labels file.\n",
        "            \"\"\"\n",
        "\n",
        "    def __init__(self, train_test_defined=False, is_train=False, combine_images=True, image_type='both', labels_set=[0,1]):\n",
        "        self.train_test_defined = train_test_defined\n",
        "        self.is_train = is_train\n",
        "        self.combine_images_into_one = combine_images\n",
        "        self.image_type = image_type\n",
        "        self.labels_set = labels_set\n",
        "\n",
        "        # Paths for train/test or all_photos\n",
        "        if train_test_defined:\n",
        "            path_tail = 'train_photos' if is_train else 'test_photos'\n",
        "            self.data_path = os.path.join(projectDirectory, path_tail)\n",
        "        else:\n",
        "\n",
        "            #for extended plastic pictures\n",
        "            self.data_path = os.path.join(projectDirectory, 'Plastic(extended)')\n",
        "            #self.data_path = os.path.join(projectDirectory, 'Plastic (full 4 by 3) augmented')\n",
        "\n",
        "\n",
        "            #self.data_path = os.path.join(projectDirectory, 'Plastic_cropped(extended)')\n",
        "            #self.data_path = os.path.join(projectDirectory, 'plastic_cropped (full) augmented')\n",
        "\n",
        "\n",
        "        # Path for labels CSV\n",
        "        self.labels_path = os.path.join(projectDirectory, 'labels_regularised_plastic_new.csv')\n",
        "\n",
        "        #only for augmentation!!\n",
        "        #self.labels_path = os.path.join(projectDirectory, 'labels_regularised_angle_augmented_final.csv')\n",
        "\n",
        "        # Load filenames and labels\n",
        "        self.image_files = self._load_file_list()\n",
        "        self.labels_dict = self._load_labels()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    # load and filter image filenames\n",
        "    # output: sorted list of image filenames\n",
        "    def _load_file_list(self):\n",
        "        image_files = []\n",
        "\n",
        "        if self.combine_images_into_one:\n",
        "          # Get all image names (without `.jpg`) in the folder\n",
        "          image_names = {f[:-4] for f in os.listdir(self.data_path) if f.endswith('.jpg')}\n",
        "          print(\"\\n[DEBUG] All image names found (without extensions):\", image_names)\n",
        "\n",
        "          # Filter only `P#` images that have a corresponding `R#` image (ensuring uniqueness)\n",
        "          paired_images = sorted(set(f\"P{num[1:]}\" for num in image_names if f\"R{num[1:]}\" in image_names))\n",
        "\n",
        "          print(\"\\n[DEBUG] Filtered paired image names (P# that has R# pair):\", paired_images)\n",
        "          print(\"\\n[DEBUG] Total paired images (should be 35 if correct):\", len(paired_images))\n",
        "\n",
        "          return paired_images  # Returns only unique `P#` images\n",
        "\n",
        "\n",
        "        for filename in os.listdir(self.data_path):\n",
        "            if filename.endswith('.jpg'):\n",
        "                if self.image_type == 'plastic' and filename.startswith('P'):\n",
        "                    image_files.append(filename)\n",
        "                elif self.image_type == 'paper' and filename.startswith('R'):\n",
        "                    image_files.append(filename)\n",
        "                elif self.image_type == 'both':\n",
        "                    image_files.append(filename)\n",
        "\n",
        "        return sorted(image_files)\n",
        "\n",
        "    # load labels from excel and map them to image filenames\n",
        "    #output: dict: keys = image names, values = label lists\n",
        "    def _load_labels(self):\n",
        "        labels_dict = {}\n",
        "        labels_df = pd.read_csv(self.labels_path)  # Read the Excel file\n",
        "\n",
        "        # Generate filenames based on the row index\n",
        "        for idx, row in labels_df.iterrows():\n",
        "            plastic_name = f\"P{idx+1:02d}\"  # P01, P02, etc.\n",
        "            paper_name = f\"R{idx+1:02d}\"    # R01, R02, etc.\n",
        "\n",
        "            # Map each filename to the corresponding labels\n",
        "            labels_dict[plastic_name] = row.tolist()\n",
        "            labels_dict[paper_name] = row.tolist()\n",
        "\n",
        "        return labels_dict\n",
        "    #inherited function from Dataset class\n",
        "    #retrieve data sample at given index\n",
        "    #output: dict containing:\n",
        "    # img (Tensor): processed image tensor\n",
        "    # label (Tensor): corresponding label tensor\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.image_files[idx]\n",
        "        base_name = os.path.splitext(image_name)[0]  # Get filename without extension\n",
        "\n",
        "        # Get label based on selected label indices\n",
        "        try:\n",
        "          label = torch.tensor([self.labels_dict[base_name][i] for i in self.labels_set])\n",
        "        except:\n",
        "          print(\"⚠️ Warning: Label not found for:\", base_name)\n",
        "          print(\"Labels available:\", self.labels_dict.get(base_name, \"No entry\"))\n",
        "          print(\"Labels set requested:\", self.labels_set)\n",
        "          return {'img': torch.zeros(6, 336, 336), 'label': torch.zeros(len(self.labels_set))}\n",
        "\n",
        "\n",
        "        # Load image(s)\n",
        "        if self.combine_images_into_one:\n",
        "            plastic_path = os.path.join(self.data_path, f'P{base_name[1:]}.jpg')  # Ensure correct extension\n",
        "            paper_path = os.path.join(self.data_path, f'R{base_name[1:]}.jpg')\n",
        "            img_plastic = self._load_image(plastic_path)\n",
        "            img_paper = self._load_image(paper_path)\n",
        "            img = torch.cat((img_plastic, img_paper), dim=0)\n",
        "        else:\n",
        "            img_path = os.path.join(self.data_path, image_name)\n",
        "            img = self._load_image(img_path)\n",
        "\n",
        "        return {'img': img, 'label': label}\n",
        "\n",
        "    #loads and preprocesses an imaage\n",
        "    #output: preprocessed image tensor\n",
        "    def _load_image(self, image_path):\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        #debug: print original image dimensions\n",
        "        #should be 4000 by 3000\n",
        "        #print(f\"[DEBUG] Original size of {os.path.basename(image_path)}: {img.size}\")  # (width, height)\n",
        "\n",
        "        #img = img.resize((336, 504))  # Resize for the model into aspect 3:2 (CROPPED)\n",
        "        #img = img.resize((672, 1008))  # Resize for the model into aspect 3:2 (CROPPED 2x)\n",
        "        #img = img.resize((168, 252))  # Resize for the model into aspect 3:2 (CROPPED 0.5x)\n",
        "        #img = img.resize((84, 126))  # Resize for the model into aspect 3:2 (CROPPED 0.5x)\n",
        "\n",
        "        img = img.resize((336, 448))  # Resize for the model into aspect 4:3 (NORMAL)\n",
        "        #img = img.resize((672, 896))  # Resize for the model into aspect 4:3 (NORMAL 2x)\n",
        "        #img = img.resize((168, 224))  # Resize for the model into aspect 4:3 (NORMAL 0.5x)\n",
        "\n",
        "        #WRONG DIMENSIONS\n",
        "        #img = img.resize((504, 336))  # Resize for the model into aspect 3:2\n",
        "        #img = img.resize((1008, 672))  # Resize for the model into aspect 3:2\n",
        "        #img = img.resize((448, 336))  # Resize for the model into aspect 4:3\n",
        "        #img = img.resize((896, 672))  # Resize for the model into aspect 4:3\n",
        "        #img = img.resize((336, 336))  # Resize for the model\n",
        "\n",
        "\n",
        "        # Normalize the image and convert to Tensor\n",
        "        normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        img_transforms = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "\n",
        "        #additional image processing can be added here if required\n",
        "\n",
        "        ###########################################################\n",
        "        return img_transforms(img)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwdpyXYU60LS"
      },
      "source": [
        "# ANN models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYoM-merxohL"
      },
      "source": [
        "## Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72rh5dS1xrb5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RMSLELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, pred, actual):\n",
        "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n",
        "\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, pred, actual):\n",
        "        return torch.sqrt(self.mse(pred, actual))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyjQIUf663lk"
      },
      "source": [
        "## AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsCiGA9365oi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from typing import Union, List, Dict, Any, cast\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes: int = 6, input_dim: int = 3) -> None:\n",
        "        print('input dimensions ', input_dim)\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            #number of  channels, number of filters, each of size__, stride, padding)\n",
        "            nn.Conv2d(input_dim, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            # nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def alexnet(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> AlexNet:\n",
        "    r\"\"\"AlexNet model architecture from the\n",
        "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
        "    The required minimum input size of the model is 63x63.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = AlexNet(**kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls['alexnet'],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0lozP4u7BB5"
      },
      "source": [
        "## VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSEdq00_7Ag2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from typing import Union, List, Dict, Any, cast\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        features: nn.Module,\n",
        "        num_classes: int = 6,\n",
        "        init_weights: bool = True\n",
        "    ) -> None:\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7)) #adapts spacial dimensions of feature map to 7 x 7\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self) -> None: #initialize weight of the conv and FCC layers at the start\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def make_layers(cfg: List[Union[str, int]], batch_norm: bool = False, input_dim: int = 3) -> nn.Sequential:\n",
        "    layers: List[nn.Module] = []\n",
        "    in_channels = input_dim\n",
        "    print('input dimensions: ', input_dim)\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            v = cast(int, v)\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfgs: Dict[str, List[Union[str, int]]] = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'F': [128, 128, 'M', 256, 256, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def _vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm, input_dim=input_dim), **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg11(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 11-layer model (configuration \"A\") from\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg11', 'A', False, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg11_bn(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg11_bn', 'A', True, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg13(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 13-layer model (configuration \"B\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg13', 'B', False, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg13_bn(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg13_bn', 'B', True, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg16(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 16-layer model (configuration \"D\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg16', 'D', False, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg16_bn(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg16_bn', 'D', True, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg19(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 19-layer model (configuration \"E\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg19', 'E', False, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg19_bn(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg19_bn', 'E', True, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "def vgg19f(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 19-layer model (configuration \"E\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg19', 'F', False, pretrained, progress, input_dim, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klsuFBj4rL_m"
      },
      "source": [
        "## Small(-er) VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7I-RVpKrN6y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from typing import Union, List, Dict, Any, cast\n",
        "\n",
        "class VGGsmall(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        features: nn.Module,\n",
        "        num_classes: int = 6,\n",
        "        init_weights: bool = True\n",
        "      ) -> None:\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self) -> None:\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def make_layers(cfg: List[Union[str, int]], batch_norm: bool = False, input_dim: int = 3) -> nn.Sequential:\n",
        "    layers: List[nn.Module] = []\n",
        "    in_channels = input_dim\n",
        "    print('input dimensions: ', input_dim)\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            v = cast(int, v)\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfgs: Dict[str, List[Union[str, int]]] = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'F': [128, 128, 'M', 256, 256, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def _vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGGsmall(make_layers(cfgs[cfg], batch_norm=batch_norm, input_dim=input_dim), **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg11(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 11-layer model (configuration \"A\") from\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg11', 'A', False, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg11_bn(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg11_bn', 'A', True, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg13(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 13-layer model (configuration \"B\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg13', 'B', False, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg13_bn(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg13_bn', 'B', True, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg16(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 16-layer model (configuration \"D\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg16', 'D', False, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg16_bn(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg16_bn', 'D', True, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg19(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 19-layer model (configuration \"E\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg19', 'E', False, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def vgg19_bn(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg19_bn', 'E', True, pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "def vgg19f(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 19-layer model (configuration \"E\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
        "    The required minimum input size of the model is 32x32.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg19', 'F', False, pretrained, progress, input_dim, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSzKySQE_HMm"
      },
      "source": [
        "## ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28u-bCcE_gKe"
      },
      "outputs": [],
      "source": [
        "!pip install pytorchcv\n",
        "!pip install pytorchcv torch>=0.4.0\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Dropout, Flatten,GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "# from .._internally_replaced_utils import load_state_dict_from_url\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
        "           'wide_resnet50_2', 'wide_resnet101_2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-b627a593.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-0676ba61.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-63fe2227.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-394f9c45.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        input_dim: int = 9\n",
        "    ) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(input_dim, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        ## FOR UNCERTAINTY-BASED MODELLING\n",
        "        #self.log_sigma1 = nn.Parameter(torch.tensor(0.2))  # For angle\n",
        "        #self.log_sigma2 = nn.Parameter(torch.tensor(-0.5))  # For velocity\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
        "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "    ## FOR UNCERTAINTY-BASED MODELLING\n",
        "    #def get_task_uncertainty(self):\n",
        "        #return self.log_sigma1, self.log_sigma2\n",
        "\n",
        "\n",
        "\n",
        "def _resnet(\n",
        "    arch: str,\n",
        "    block: Type[Union[BasicBlock, Bottleneck]],\n",
        "    layers: List[int],\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    input_dim: int = 3,\n",
        "    **kwargs: Any\n",
        ") -> ResNet:\n",
        "    model = ResNet(block, layers, input_dim=input_dim, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-50 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-101 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "    Args:\n",
        "        pretrained (bol): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-152 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNeXt-50 32x4d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 4\n",
        "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def resnext101_32x8d(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNeXt-101 32x8d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 8\n",
        "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
        "                   pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def wide_resnet50_2(pretrained: bool = False, progress: bool = True, input_dim: int = 3, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"Wide ResNet-50-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
        "                   pretrained, progress, input_dim, **kwargs)\n",
        "\n",
        "\n",
        "def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"Wide ResNet-101-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "\n",
        "from pytorchcv.model_provider import get_model as ptcv_get_model\n",
        "net = ptcv_get_model(\"resnet18\", pretrained=True)\n",
        "\n",
        "from torch.autograd import Variable\n",
        "x = Variable(torch.randn(1, 3, 224, 224))\n",
        "\n",
        "y = net(x)\n",
        "\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w81J9aAPwRxo"
      },
      "source": [
        "# Train and Test functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv3eHF46hitq"
      },
      "source": [
        "### CNN-specific parameters:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXADqWWkhitr"
      },
      "source": [
        "#### optimizer: Defines how the model updates its weights during training.\n",
        "        Options include:\n",
        "            - Adam (used here): Adaptive learning rate optimization, generally fast and efficient.\n",
        "            - SGD (Stochastic Gradient Descent): Slower but sometimes more robust for large datasets.\n",
        "\n",
        "\n",
        "### loss_func: Specifies the function used to compute the difference between predictions and ground truth.\n",
        "        * MSELoss: Mean Squared Error, suitable for regression tasks.\n",
        "        * L1Loss: Mean Absolute Error, used here for validation.\n",
        "        * Custom losses (e.g., RMSE) can be substituted based on task requirements.\n",
        "\n",
        "\n",
        "### scheduler: Dynamically adjusts the learning rate during training.\n",
        "        * ReduceLROnPlateau: Lowers the learning rate if the validation loss plateaus, helping to fine-tune the model during later epochs.\n",
        "\n",
        "\n",
        "### num_epoch: Total number of epochs for training.\n",
        "        * Each epoch represents one complete pass through the training dataset.\n",
        "\n",
        "### batch_size: Number of samples processed together in a single forward/backward pass.\n",
        "        * Affects memory usage and training stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0crwM_WFdcj"
      },
      "source": [
        "### look at kfold spliting indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6ujIK8GFirL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Simulating your dataset with 57 samples (just numbers 0 to 56)\n",
        "num_samples = 57\n",
        "dataset_indices = np.arange(num_samples)\n",
        "\n",
        "# Define KFold parameters\n",
        "k = 3  # Number of folds\n",
        "kfold = KFold(n_splits=k, shuffle=True, random_state=42)  # Ensure deterministic splits\n",
        "\n",
        "# Print the split indices\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset_indices)):\n",
        "    print(f\"Fold {fold + 1}:\")\n",
        "    print(f\"Train Indices: {train_idx}\")\n",
        "    print(f\"Validation Indices: {val_idx}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkU8ZlwCj5Ia"
      },
      "source": [
        "### Train and Test with MSE printed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jgq5LgMj8n0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def labeled_kfold_train_with_mse(model_str, num_epoch=100, batch_size=4, labels_set=[], k=4, verbose=True, save_dir=None, combine_images=False, image_type='both'):\n",
        "    #Perform k-fold cross-validation for training a PyTorch model with MSE and MAE for validation.\n",
        "    uncertain = False\n",
        "\n",
        "    # Initialize dataset\n",
        "    print(f\"Initializing dataset with combin_images as {combine_images}\")\n",
        "    dataset = LabeledGlassSet(\n",
        "        train_test_defined=False,\n",
        "        is_train=False,\n",
        "        combine_images=combine_images,\n",
        "        image_type=image_type,\n",
        "        labels_set=labels_set\n",
        "    )\n",
        "\n",
        "    print(f\"Total dataset items: {len(dataset)}\")\n",
        "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    if save_dir is not None and k > 2:\n",
        "        base_save_dir = save_dir.split('.')[0]  # remove .pt\n",
        "        os.makedirs(base_save_dir, exist_ok=True)\n",
        "    else:\n",
        "        base_save_dir = None\n",
        "\n",
        "    print(\"Starting k-fold cross-validation...\")\n",
        "\n",
        "    # Prepare dictionary to store loss values\n",
        "    results_stats = {\"Epoch\": list(range(1, num_epoch + 1))}\n",
        "    for f in range(k):\n",
        "        results_stats[f\"Fold {f + 1} - Train Loss\"] = [None] * num_epoch\n",
        "        results_stats[f\"Fold {f + 1} - Val Loss (MAE)\"] = [None] * num_epoch\n",
        "        results_stats[f\"Fold {f + 1} - Val Loss (MSE)\"] = [None] * num_epoch\n",
        "\n",
        "    all_folds = list(enumerate(kfold.split(dataset)))\n",
        "\n",
        "    fold_count = 0\n",
        "\n",
        "    for fold, (train_idx, val_idx) in all_folds:\n",
        "\n",
        "        print(f\"\\nFold {fold + 1}/{k}\")\n",
        "\n",
        "        # Split dataset into training and validation subsets\n",
        "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
        "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_subset, batch_size=batch_size, shuffle=True, pin_memory=True\n",
        "        )\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            val_subset, batch_size=batch_size, shuffle=False, pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Initialize model, optimizer, and scheduler\n",
        "        learning_rate = 1e-5\n",
        "        factor = 0.5\n",
        "        patience = 5\n",
        "        threshold = 1e-4\n",
        "        min_lr = 1e-6\n",
        "        print(f\"Learning rate: {learning_rate}\")\n",
        "        print(f\"Factor: {factor}, Patience: {patience}, Threshold: {threshold}, Min LR: {min_lr}\")\n",
        "\n",
        "        model = get_model(model_str)\n",
        "        my_model = model.cuda().float()\n",
        "\n",
        "\n",
        "        #CURRENT OPTIMISATION FUNCTION: ADAM\n",
        "        optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "        # CURRENT LOSS_FUNCTION\n",
        "        loss_func = torch.nn.MSELoss()  # Training loss (MSE)\n",
        "\n",
        "        val_loss_func = torch.nn.L1Loss()   # Validation loss (MAE)\n",
        "        val_loss_func_mse = torch.nn.MSELoss()  # Validation loss (MSE)\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, 'min', factor=factor, patience=patience, threshold=threshold, verbose=True, min_lr=min_lr\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        min_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epoch):\n",
        "            # ===========================\n",
        "            #  Training Phase\n",
        "            # ===========================\n",
        "            my_model.train()\n",
        "            total_train_loss = 0\n",
        "            for data in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                labels = data['label'].cuda().float()\n",
        "                img = data['img'].cuda().float()\n",
        "\n",
        "                # Prediction\n",
        "                prediction = my_model(img)\n",
        "                #prediction = my_model(img).clamp(0, 1)  # Clamping predictions\n",
        "\n",
        "                # Compute MSE Loss\n",
        "                # CAN MODIFY TO INCLUDE UNCERTAINTY\n",
        "                if not uncertain:\n",
        "                  loss = loss_func(prediction, labels.reshape(prediction.shape))\n",
        "\n",
        "                else:\n",
        "                  #uncertainty\n",
        "                  \"\"\"angle_pred = prediction[:, 0]\n",
        "                  vel_pred = prediction[:, 1]\n",
        "                  angle_true = labels[:, 0]\n",
        "                  vel_true = labels[:, 1]\n",
        "\n",
        "                  angle_loss = torch.nn.functional.mse_loss(angle_pred, angle_true)\n",
        "                  vel_loss = torch.nn.functional.mse_loss(vel_pred, vel_true)\n",
        "\n",
        "                  sigma1 = torch.exp(my_model.log_sigma1)\n",
        "                  sigma2 = torch.exp(my_model.log_sigma2)\n",
        "                  constant = 100\n",
        "\n",
        "                  loss = (1*constant / (2 * sigma1**2)) * angle_loss + torch.log(sigma1) + \\\n",
        "                        (1*constant / (2 * sigma2**2)) * vel_loss + torch.log(sigma2)\"\"\"\n",
        "                  #######################################################################\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            # ===========================\n",
        "            #  Validation Phase\n",
        "            # ===========================\n",
        "            my_model.eval()\n",
        "            total_val_loss = 0\n",
        "            total_val_loss_mse = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for data in val_loader:\n",
        "                    labels = data['label'].cuda().float()\n",
        "                    img = data['img'].cuda().float()\n",
        "\n",
        "                    prediction = my_model(img).clamp(0, 1)  # Clamp predictions to [0, 1]\n",
        "\n",
        "                    # Compute MAE and MSE for validation\n",
        "                    val_loss = val_loss_func(prediction, labels.reshape(prediction.shape))\n",
        "                    val_loss_mse = val_loss_func_mse(prediction, labels.reshape(prediction.shape))\n",
        "\n",
        "                    total_val_loss += val_loss.item()\n",
        "                    total_val_loss_mse += val_loss_mse.item()\n",
        "\n",
        "                    # Sanity Checks\n",
        "                    if epoch % 10 == 0 and len(val_loader) < 5:\n",
        "                        print(f\"Val Sample Check - Epoch {epoch + 1}\")\n",
        "                        print(f\"Predictions range: {prediction.min().item()} to {prediction.max().item()}\")\n",
        "                        print(f\"Labels range: {labels.min().item()} to {labels.max().item()}\")\n",
        "\n",
        "            avg_val_loss = total_val_loss / len(val_loader)  # Average MAE\n",
        "            avg_val_loss_mse = total_val_loss_mse / len(val_loader)  # Average MSE\n",
        "\n",
        "            # ===========================\n",
        "            #  Logging and Reporting\n",
        "            # ===========================\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "            results_stats[f\"Fold {fold + 1} - Train Loss\"][epoch] = avg_train_loss\n",
        "            results_stats[f\"Fold {fold + 1} - Val Loss (MAE)\"][epoch] = avg_val_loss\n",
        "            results_stats[f\"Fold {fold + 1} - Val Loss (MSE)\"][epoch] = avg_val_loss_mse\n",
        "\n",
        "            if verbose:\n",
        "                if not uncertain:\n",
        "                    print(f\"Epoch {epoch + 1}/{num_epoch}: \"\n",
        "                          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "                          f\"Val Loss (MAE): {avg_val_loss:.4f}, \"\n",
        "                          f\"Val Loss (MSE): {avg_val_loss_mse:.4f}\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch + 1}/{num_epoch}: \"\n",
        "                    f\"Total Loss (MSE): {avg_train_loss:.4f}, \"\n",
        "                    f\"Angle Loss (MSE): {angle_loss.item():.4f}, \"\n",
        "                    f\"Velocity Loss (MSE): {vel_loss.item():.4f}, \"\n",
        "                    f\"σ1: {sigma1.item():.4f}, σ2: {sigma2.item():.4f}\")\n",
        "\n",
        "            # ===========================\n",
        "            # Save Best Model\n",
        "            # ===========================\n",
        "            if avg_val_loss < min_val_loss:\n",
        "                min_val_loss = avg_val_loss\n",
        "                if save_dir is not None:\n",
        "                    if k > 2:\n",
        "                        fold_save_path = os.path.join(base_save_dir, f'fold_{fold + 1}_best_model.pt')\n",
        "                    else:\n",
        "                        fold_save_path = save_dir\n",
        "                    torch.save(my_model.state_dict(), fold_save_path)\n",
        "                    print(f\" Model updated in epoch {epoch + 1}, saved to {fold_save_path}\")\n",
        "\n",
        "        print(f\"Fold {fold + 1} Final Validation Loss (MAE): {min_val_loss:.4f}\")\n",
        "        ###########################################################################\n",
        "\n",
        "        ############################################################################\n",
        "        fold_results.append(min_val_loss)\n",
        "\n",
        "    # ===========================\n",
        "    # Save Final Results to CSV\n",
        "    # ===========================\n",
        "    df_results = pd.DataFrame(results_stats)\n",
        "    df_results.to_csv(os.path.join(base_save_dir, 'results_with_mse.csv'), index=False)\n",
        "    avg_loss = sum(fold_results) / len(fold_results)\n",
        "    print(f\"\\n🔥 Average Validation Loss Across Folds (MAE): {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def labeled_test(model, batch_size=1, labels_set=[], combine_images=True, image_type='both', save_path=\"test_results.xlsx\",  want_normalise=False):\n",
        "    \"\"\"\n",
        "    Tests the model using the LabeledGlassSet dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The PyTorch model to be tested.\n",
        "    - batch_size (int): Batch size for testing.\n",
        "    - labels_set (list of int): Indices of labels to use for testing.\n",
        "    - combine_images (bool): Whether to combine plastic and paper images into one tensor.\n",
        "    - image_type (str): Specify image type when not combining ('plastic', 'paper', or 'both').\n",
        "\n",
        "    Returns:\n",
        "    - avg_test_loss (float): Average L1 test loss.\n",
        "    \"\"\"\n",
        "    my_model = model.cuda().eval()  # Move to GPU & Set to evaluation mode\n",
        "\n",
        "    # Initialize the test dataset\n",
        "    test_set = LabeledGlassSet(\n",
        "        train_test_defined=False,  # Use the full dataset\n",
        "        is_train=False,            # This is the test dataset\n",
        "        combine_images=combine_images,\n",
        "        image_type=image_type,\n",
        "        labels_set=labels_set\n",
        "    )\n",
        "\n",
        "    print(f\"Total test items: {len(test_set)}\")\n",
        "\n",
        "    # Create the DataLoader for the test dataset\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_set, batch_size=batch_size, shuffle=False, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize metrics\n",
        "    total_samples = 0\n",
        "    total_loss = 0\n",
        "    loss_func = torch.nn.L1Loss()  # L1 loss for evaluation\n",
        "\n",
        "    results = []\n",
        "\n",
        "    def denormalise(input):\n",
        "        original_min = np.array([0,4.508])\n",
        "        original_max =np.array([27.5, 5.77])\n",
        "        return (input * (original_max - original_min)) + original_min\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            labels = data['label'].cuda().float()\n",
        "            img = data['img'].cuda().float()\n",
        "\n",
        "            # Get model predictions\n",
        "            prediction = my_model(img)\n",
        "\n",
        "            # Compute L1 loss\n",
        "            loss = loss_func(prediction, labels.reshape(prediction.shape))\n",
        "            total_samples += labels.size(0)\n",
        "            total_loss += loss.item()\n",
        "            #whether to denormalise the prediction and actual results\n",
        "\n",
        "            # Normalized values\n",
        "            pred_norm = prediction.cpu().numpy()\n",
        "            actual_norm = labels.cpu().numpy()\n",
        "\n",
        "            # Denormalized values\n",
        "            pred_denorm = denormalise(pred_norm)\n",
        "            actual_denorm = denormalise(actual_norm)\n",
        "\n",
        "            # Store in results\n",
        "            for j in range(len(pred_norm)):\n",
        "                results.append({\n",
        "                    \"Test Item\": i * batch_size + j + 1,\n",
        "                    \"Actual Angle (Normalized)\": actual_norm[j][0],\n",
        "                    \"Actual Velocity (Normalized)\": actual_norm[j][1],\n",
        "                    \"Predicted Angle (Normalized)\": pred_norm[j][0],\n",
        "                    \"Predicted Velocity (Normalized)\": pred_norm[j][1],\n",
        "                    \"Actual Angle (Denormalized)\": actual_denorm[j][0],\n",
        "                    \"Actual Velocity (Denormalized)\": actual_denorm[j][1],\n",
        "                    \"Predicted Angle (Denormalized)\": pred_denorm[j][0],\n",
        "                    \"Predicted Velocity (Denormalized)\": pred_denorm[j][1]\n",
        "                })\n",
        "            # Print results for each test sample\n",
        "            print(f'--Test item {i:5d}')\n",
        "            if want_normalise:\n",
        "                print('Prediction Normalized: {}'.format(pred_norm))\n",
        "                print('Actual Normalized:     {}\\n'.format(actual_norm))\n",
        "            else:\n",
        "                print('Prediction Denormalized: {}'.format(pred_denorm))\n",
        "                print('Actual Denormalized:     {}\\n'.format(actual_denorm))\n",
        "\n",
        "    # Compute and print average test loss\n",
        "    avg_test_loss = total_loss / total_samples\n",
        "    print('Final Test L1 Loss: {:.3f}'.format(avg_test_loss))\n",
        "\n",
        "    # Convert results to a DataFrame and save to Excel\n",
        "    df_results = pd.DataFrame(results)\n",
        "    df_results.to_excel(save_path, index=False)\n",
        "\n",
        "    print(f\"Results saved to {save_path}\")\n",
        "\n",
        "    return avg_test_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_kfold_models(model_str, labels_set=[0,1], k=3, batch_size=1, base_save_dir='', combine_images=False, image_type='plastic', denormalise_results=True):\n",
        "    dataset = LabeledGlassSet(\n",
        "        train_test_defined=False,\n",
        "        is_train=False,\n",
        "        combine_images=combine_images,\n",
        "        image_type=image_type,\n",
        "        labels_set=labels_set\n",
        "    )\n",
        "\n",
        "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    def denormalise(input_tensor):\n",
        "        original_min = np.array([0, 4.508])\n",
        "        original_max = np.array([27.5, 5.77])\n",
        "        return (input_tensor * (original_max - original_min)) + original_min\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for fold, (_, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print(f\"\\n Evaluating Fold {fold + 1}/{k}...\")\n",
        "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
        "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "        model = get_model(model_str)\n",
        "        model.load_state_dict(torch.load(os.path.join(base_save_dir, f'fold_{fold + 1}_best_model.pt')))\n",
        "        model = model.cuda().eval()\n",
        "\n",
        "        total_angle_mae, total_velocity_mae, total_samples = 0, 0, 0\n",
        "        val_samples = []  # To store sample info\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(val_loader):\n",
        "                labels = batch['label'].cuda().float()\n",
        "                images = batch['img'].cuda().float()\n",
        "                #preds = model(images).clamp(0, 1)\n",
        "                preds = model(images)\n",
        "\n",
        "                preds_np = preds.cpu().numpy()\n",
        "                labels_np = labels.cpu().numpy()\n",
        "\n",
        "                if denormalise_results:\n",
        "                    preds_np = denormalise(preds_np)\n",
        "                    labels_np = denormalise(labels_np)\n",
        "\n",
        "                angle_mae = np.abs(preds_np[:, 0] - labels_np[:, 0]).sum()\n",
        "                velocity_mae = np.abs(preds_np[:, 1] - labels_np[:, 1]).sum()\n",
        "\n",
        "                total_angle_mae += angle_mae\n",
        "                total_velocity_mae += velocity_mae\n",
        "                total_samples += labels.size(0)\n",
        "\n",
        "                # Grab index from original dataset\n",
        "                original_index = val_idx[i]\n",
        "                sample = dataset[original_index]\n",
        "\n",
        "                val_samples.append({\n",
        "                    \"Index\": original_index,\n",
        "                    \"Filename\": sample.get(\"filename\", \"N/A\") if isinstance(sample, dict) else \"N/A\",\n",
        "                    \"True Labels\": labels_np[0],\n",
        "                    \"Predicted Labels\": preds_np[0],\n",
        "                })\n",
        "\n",
        "        # Print per-sample info\n",
        "        print(f\"\\n Fold {fold + 1} - Validation Samples:\")\n",
        "        for item in val_samples:\n",
        "            print(f\"  Index: {item['Index']:4d} | File: {item['Filename']} | \"\n",
        "                  f\"True: {item['True Labels']} | Pred: {item['Predicted Labels']}\")\n",
        "\n",
        "        fold_result = {\n",
        "            \"Fold\": fold + 1,\n",
        "            \"Angle MAE\": total_angle_mae / total_samples,\n",
        "            \"Velocity MAE\": total_velocity_mae / total_samples,\n",
        "            \"Overall MAE\": (total_angle_mae + total_velocity_mae) / (2 * total_samples)\n",
        "        }\n",
        "        print(f\"\\n Fold {fold + 1} Results: {fold_result}\")\n",
        "        results.append(fold_result)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n"
      ],
      "metadata": {
        "id": "e9fbqRqKOju8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model testing: generate statistics for validation MAE loss of each sample\n",
        "\n",
        "main function used to test the k fold models\n",
        "\n",
        "for each fold of the same model, compute the MAE validation loss of every data sample in its respective validation split set\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fSZbETvh2qPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def test_kfold_models2(model_str, labels_set=[0,1], k=3, batch_size=1, base_save_dir='', combine_images=False, image_type='plastic', clamp=False, augmenting = False):\n",
        "    dataset = LabeledGlassSet(\n",
        "        train_test_defined=False,\n",
        "        is_train=False,\n",
        "        combine_images=combine_images,\n",
        "        image_type=image_type,\n",
        "        labels_set=labels_set\n",
        "    )\n",
        "\n",
        "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    def denormalise(input_tensor, augmenting):\n",
        "        if augmenting:\n",
        "          original_min = np.array([-27.5, 4.508])\n",
        "        else:\n",
        "          original_min = np.array([0, 4.508])\n",
        "        original_max = np.array([27.5, 5.77])\n",
        "        return (input_tensor * (original_max - original_min)) + original_min\n",
        "\n",
        "    all_results = []\n",
        "    norm_summary = []\n",
        "    denorm_summary = []\n",
        "\n",
        "    for fold, (_, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print(f\"\\n Evaluating Fold {fold + 1}/{k}...\")\n",
        "        val_subset = Subset(dataset, val_idx)\n",
        "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "        model = get_model(model_str)\n",
        "        model.load_state_dict(torch.load(os.path.join(base_save_dir, f'fold_{fold + 1}_best_model.pt')))\n",
        "        model = model.cuda().eval()\n",
        "\n",
        "        total_angle_mae_norm, total_velocity_mae_norm = 0, 0\n",
        "        total_angle_mae_denorm, total_velocity_mae_denorm = 0, 0\n",
        "        total_samples = 0\n",
        "\n",
        "        per_sample_records = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(val_loader):\n",
        "                labels = batch['label'].cuda().float()\n",
        "                images = batch['img'].cuda().float()\n",
        "\n",
        "                preds = model(images).clamp(0,1) if clamp else model(images)\n",
        "\n",
        "                preds_np = preds.cpu().numpy()\n",
        "                labels_np = labels.cpu().numpy()\n",
        "\n",
        "                denorm_preds = denormalise(preds_np, augmenting)\n",
        "                denorm_labels = denormalise(labels_np, augmenting)\n",
        "\n",
        "                angle_err_norm = np.abs(preds_np[:, 0] - labels_np[:, 0])\n",
        "                velocity_err_norm = np.abs(preds_np[:, 1] - labels_np[:, 1])\n",
        "\n",
        "                angle_err_denorm = np.abs(denorm_preds[:, 0] - denorm_labels[:, 0])\n",
        "                velocity_err_denorm = np.abs(denorm_preds[:, 1] - denorm_labels[:, 1])\n",
        "\n",
        "                for j in range(len(preds_np)):\n",
        "                    original_index = val_idx[i * batch_size + j]\n",
        "                    sample = dataset[original_index]\n",
        "                    filename = sample.get(\"filename\", \"N/A\") if isinstance(sample, dict) else \"N/A\"\n",
        "\n",
        "                    per_sample_records.append({\n",
        "                        \"Fold\": fold + 1,\n",
        "                        \"Index\": original_index + 1,\n",
        "                        \"True Angle (Norm)\": labels_np[j][0],\n",
        "                        \"True Velocity (Norm)\": labels_np[j][1],\n",
        "                        \"Pred Angle (Norm)\": preds_np[j][0],\n",
        "                        \"Pred Velocity (Norm)\": preds_np[j][1],\n",
        "                        \"Angle AbsErr (Norm)\": angle_err_norm[j],\n",
        "                        \"Velocity AbsErr (Norm)\": velocity_err_norm[j],\n",
        "                        \"True Angle (Denorm)\": denorm_labels[j][0],\n",
        "                        \"True Velocity (Denorm)\": denorm_labels[j][1],\n",
        "                        \"Pred Angle (Denorm)\": denorm_preds[j][0],\n",
        "                        \"Pred Velocity (Denorm)\": denorm_preds[j][1],\n",
        "                        \"Angle AbsErr (Denorm)\": angle_err_denorm[j],\n",
        "                        \"Velocity AbsErr (Denorm)\": velocity_err_denorm[j],\n",
        "                    })\n",
        "\n",
        "                    total_angle_mae_norm += angle_err_norm[j]\n",
        "                    total_velocity_mae_norm += velocity_err_norm[j]\n",
        "                    total_angle_mae_denorm += angle_err_denorm[j]\n",
        "                    total_velocity_mae_denorm += velocity_err_denorm[j]\n",
        "\n",
        "                total_samples += len(preds_np)\n",
        "\n",
        "        norm_summary.append([\n",
        "            total_angle_mae_norm / total_samples,\n",
        "            total_velocity_mae_norm / total_samples,\n",
        "            (total_angle_mae_norm + total_velocity_mae_norm) / (2 * total_samples)\n",
        "        ])\n",
        "        denorm_summary.append([\n",
        "            total_angle_mae_denorm / total_samples,\n",
        "            total_velocity_mae_denorm / total_samples,\n",
        "            (total_angle_mae_denorm + total_velocity_mae_denorm) / (2 * total_samples)\n",
        "        ])\n",
        "\n",
        "        all_results.extend(per_sample_records)\n",
        "\n",
        "    # Save to Excel\n",
        "    df_all = pd.DataFrame(all_results)\n",
        "    filename = \"kfold_validation_full_results\"\n",
        "    if clamp:\n",
        "        filename += \"_clamped\"\n",
        "    filename += \".xlsx\"\n",
        "    save_path = os.path.join(base_save_dir, filename)\n",
        "    df_all.to_excel(save_path, index=False)\n",
        "    print(f\"\\n All fold results saved to {save_path}\")\n",
        "\n",
        "    # Print Summary Tables\n",
        "    summary_norm = pd.DataFrame(norm_summary, columns=[\"Angle MAE\", \"Velocity MAE\", \"Overall MAE\"],\n",
        "                                index=[f\"Fold {i+1}\" for i in range(k)])\n",
        "    summary_denorm = pd.DataFrame(denorm_summary, columns=[\"Angle MAE\", \"Velocity MAE\", \"Overall MAE\"],\n",
        "                                  index=[f\"Fold {i+1}\" for i in range(k)])\n",
        "\n",
        "    print(\"\\n Normalized MAE Summary:\")\n",
        "    print(summary_norm.round(4).to_markdown())\n",
        "\n",
        "    print(\"\\n Denormalized MAE Summary:\")\n",
        "    print(summary_denorm.round(4).to_markdown())\n",
        "\n",
        "    return df_all"
      ],
      "metadata": {
        "id": "7IQM4PV12yOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_str = 'resnext101_32x8d'\n",
        "labels_set = [0, 1]\n",
        "k = 3\n",
        "batch_size = 1\n",
        "base_save_dir = 'trained_models/resnext101_32x8d_3_fold_adam_single_plastic_cropped_data_aug_3'  # Folder where your models are saved\n",
        "combine_images = False\n",
        "image_type = 'plastic'\n",
        "clamp = False\n",
        "augmenting = True\n",
        "\n",
        "if not default_lock:\n",
        "    test_kfold_models2(model_str = model_str, labels_set= labels_set, k=k, batch_size=batch_size, base_save_dir= base_save_dir, combine_images= combine_images, image_type= image_type, clamp = clamp, augmenting= augmenting)\n",
        "    clamp = True\n",
        "    test_kfold_models2(model_str = model_str, labels_set= labels_set, k=k, batch_size=batch_size, base_save_dir= base_save_dir, combine_images= combine_images, image_type= image_type, clamp = clamp, augmenting = augmenting)\n",
        "\n"
      ],
      "metadata": {
        "id": "fLcwmT114268"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# look at the current validation split\n"
      ],
      "metadata": {
        "id": "uiB0PUzE5M3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def log_kfold_validation_info(dataset, k=4, output_path=None):\n",
        "    \"\"\"\n",
        "    Logs the filenames and label parameters used for validation in each k-fold.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: The full dataset used in k-fold cross-validation.\n",
        "    - k (int): Number of folds.\n",
        "    - output_path (str or None): If provided, saves the log to a CSV file at this path.\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import KFold\n",
        "    import pandas as pd\n",
        "\n",
        "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    fold_logs = []\n",
        "\n",
        "    for fold_idx, (_, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        for i in val_idx:\n",
        "            sample = dataset[i]\n",
        "            label = sample['label']\n",
        "            img_name = dataset.image_files[i]\n",
        "            log_entry = {\n",
        "                \"Fold\": fold_idx + 1,\n",
        "                \"Index\": i+1,\n",
        "                \"img_name\": img_name,\n",
        "                \"Label\": label\n",
        "            }\n",
        "            fold_logs.append(log_entry)\n",
        "\n",
        "    df = pd.DataFrame(fold_logs)\n",
        "\n",
        "    if output_path:\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"Validation info saved to {output_path}\")\n",
        "    else:\n",
        "        print(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "dataset = LabeledGlassSet(\n",
        "        train_test_defined=False,\n",
        "        is_train=False,\n",
        "        combine_images=False,\n",
        "        image_type='plastic',\n",
        "        labels_set=[0,1]\n",
        "    )\n",
        "\n",
        "log_kfold_validation_info(dataset, k=3, output_path= False)\n"
      ],
      "metadata": {
        "id": "qF4Pi9595Q8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pcLFOCKXVGH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XubEGoeDwXk4"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDNu2FME13Uh"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9Bk-XjG_4Nb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dictionary of the different cnn models that can be used\n",
        "NN_FMAP = {\n",
        "    'alexnet': alexnet,\n",
        "    'vgg11': vgg11,\n",
        "    'vgg11_bn': vgg11_bn,\n",
        "    'vgg13': vgg13,\n",
        "    'vgg13_bn': vgg13_bn,\n",
        "    'vgg16': vgg16,\n",
        "    'vgg16_bn': vgg16_bn,\n",
        "    'vgg19': vgg19,\n",
        "    'vgg19_bn': vgg19_bn,\n",
        "    'vgg19f': vgg19f,\n",
        "    'resnext101_32x8d': resnext101_32x8d,\n",
        "    'wide_resnet50_2': wide_resnet50_2,\n",
        "}\n",
        "NUM_PARAMS = 2 #adjust according to number of parameters used\n",
        "INPUT_DIM = 3 #adjust according to whetehr we combining or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1tiNunT_tkq"
      },
      "outputs": [],
      "source": [
        "def get_model(model_type, num_classes=NUM_PARAMS):\n",
        "    if model_type not in NN_FMAP:\n",
        "        print('Unsupported models. Supported models: %'\n",
        "            % NN_FMAP.keys())\n",
        "        return None\n",
        "\n",
        "    func = NN_FMAP[model_type]\n",
        "\n",
        "    return func(num_classes=num_classes, input_dim=INPUT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayh7-kedx2gd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bPSQZWdNA9P"
      },
      "source": [
        "## visualise the image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4lO5RVZNFL0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = LabeledGlassSet(\n",
        "    train_test_defined=False,\n",
        "    is_train=False,\n",
        "    combine_images=False,  # Ensure we are loading combined images\n",
        "    image_type='plastic',\n",
        "    labels_set=[0, 1]  # Adjust based on your needs\n",
        ")\n",
        "for i in range(0,20):\n",
        "    # Pick a sample from the dataset\n",
        "    index = i  # Change index to visualize different images\n",
        "    sample = dataset[i]\n",
        "    img_tensor = sample['img']\n",
        "    label = sample['label']\n",
        "\n",
        "    # Get corresponding image filenames\n",
        "    print(f'index', index)\n",
        "    print(f'label', label)\n",
        "\n",
        "    img_name = dataset.image_files[index]  # Since only P# filenames are stored\n",
        "    print(f'img_name', img_name)\n",
        "\n",
        "    # Extract image number\n",
        "    image_number = img_name[1:]  # Removes 'P' to get \"01\", \"02\", etc.\n",
        "\n",
        "    # Check shape of tensor\n",
        "    print(\"Image Tensor Shape:\", img_tensor.shape)  # Should be (6, H, W)\n",
        "\n",
        "    # If images were normalized with mean=0.5, std=0.5, unnormalize\n",
        "    unnormalize = lambda x: x * 0.5 + 0.5\n",
        "    img_tensor = unnormalize(img_tensor)\n",
        "\n",
        "    if img_tensor.shape[0] == 6:\n",
        "      # Split the 6-channel tensor into two 3-channel tensors\n",
        "      img_plastic = img_tensor[:3, :, :]  # First 3 channels\n",
        "      img_paper = img_tensor[3:, :, :]  # Last 3 channels\n",
        "\n",
        "      # Convert tensors to numpy\n",
        "      img_plastic_np = img_plastic.permute(1, 2, 0).cpu().numpy()\n",
        "      img_paper_np = img_paper.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "      # Plot both images with the correct filename\n",
        "      fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "      ax[0].imshow(img_plastic_np)\n",
        "      ax[0].set_title(f\"Plastic Image (P{image_number+1})\")  # Shows correct number\n",
        "      ax[0].axis(\"off\")\n",
        "\n",
        "      ax[1].imshow(img_paper_np)\n",
        "      ax[1].set_title(f\"Paper Image (R{image_number+1})\")  # Shows correct number\n",
        "      ax[1].axis(\"off\")\n",
        "\n",
        "      plt.suptitle(f\"Label: {label.numpy()}\")\n",
        "      plt.show()\n",
        "\n",
        "    elif img_tensor.shape[0] == 3:\n",
        "\n",
        "      # Convert tensors to numpy\n",
        "      img_plastic_np = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "      # Plot both images with the correct filename\n",
        "      fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "      ax[0].imshow(img_plastic_np)\n",
        "      ax[0].set_title(f\"Plastic Image (P{image_number})\")  # Shows correct number\n",
        "      ax[0].axis(\"off\")\n",
        "\n",
        "      plt.suptitle(f\"Label: {label.numpy()}\")\n",
        "      plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unuy5f0XRzbd"
      },
      "source": [
        "## Train ANN with custom labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXjbNaBSRy8I"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Constants\n",
        "model_str = 'resnext101_32x8d'  # Model architecture\n",
        "num_epoch, batch_size = 60, 4  # Reduced epochs for faster testing\n",
        "labels_set = [0, 1]  # Labels used for training (0: angle, 1: velocity, 2: kinetic energy)\n",
        "combine_images = False  # Whether to combine plastic and paper images into 1 tensor\n",
        "image_type = 'plastic'  # Use both image types\n",
        "use_kfold = True  # Set to True for k-fold training\n",
        "k_folds = 3  # Number of folds for k-fold training\n",
        "# Define model name based on parameters\n",
        "\n",
        "model_name = f\"{model_str}_{f'{k_folds}_fold' if use_kfold else 'standard'}_SGD_single_plastic_.pt\"\n",
        "save_dir = os.path.join(\"trained_models\", model_name)\n",
        "\n",
        "# Print training details\n",
        "print(f\"Model: {model_str}\")\n",
        "print(f\"Save Path: {save_dir}\")\n",
        "print(f\"Training Method: {'K-Fold' if use_kfold else 'Standard'}\")\n",
        "\n",
        "# Start training\n",
        "t1 = time.time()\n",
        "if use_kfold:\n",
        "    # K-Fold Training\n",
        "    avg_val_loss = labeled_kfold_train_with_mse(\n",
        "        model_str=model_str,\n",
        "        num_epoch=num_epoch,\n",
        "        batch_size=batch_size,\n",
        "        labels_set=labels_set,\n",
        "        k=k_folds,\n",
        "        verbose=True,\n",
        "        save_dir=save_dir, #resnext101_32x8d_4_fold_adam_336.pt\n",
        "        combine_images=combine_images,\n",
        "        image_type=image_type\n",
        "    )\n",
        "    print(f\"Average Validation Loss Across Folds: {avg_val_loss:.4f}\")\n",
        "else:\n",
        "    pass\n",
        "t2 = time.time()\n",
        "\n",
        "# Print training time\n",
        "training_time = t2 - t1\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avas4C8S6Jlx"
      },
      "source": [
        "##Analysis of Training/Validation loss (val MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbYo7n3s6Lk3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "dic = {1: \"R-060-1x-mid\",\n",
        "       2:\"R-060-1x-mid-c\",\n",
        "       3:\"R-060-2x-mid\",\n",
        "       4:\"R-060-2x-mid-c\",\n",
        "       5:\"R-120-1x-mid\",\n",
        "       6:\"R-120-1x-mid-c\",\n",
        "       7:\"R-120-1x-low\",\n",
        "       8:\"R-120-1x-low-c\",\n",
        "       9:\"R-aug\",\n",
        "       10:\"R-aug-c\"\n",
        "}\n",
        "\n",
        "def show_mse_statistics(model_data, i):\n",
        "  print(model_data)\n",
        "  df = pd.read_csv(model_data)\n",
        "\n",
        "  # Define folds & column names\n",
        "  folds = [\"Fold 1\", \"Fold 2\", \"Fold 3\"]\n",
        "  #folds = [\"Fold 1\", \"Fold 2\", \"Fold 3\", \"Fold 4\"]\n",
        "  train_cols = [f\"{fold} - Train Loss\" for fold in folds]\n",
        "  val_cols = [f\"{fold} - Val Loss (MSE)\" for fold in folds]\n",
        "\n",
        "  for fold, train_col, val_col in zip(folds, train_cols, val_cols):\n",
        "    plt.figure(figsize=(10,8)) #width by height\n",
        "    num_epochs = len(df)\n",
        "    if num_epochs == 120:\n",
        "      plt.figure(figsize=(20,8))\n",
        "\n",
        "\n",
        "    #set epoches limit\n",
        "    plt.ylim(0, 0.4)\n",
        "    plt.xlim(0, len(df))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    plt.plot(df['Epoch'], df[train_col], linestyle=\"--\", label=f\"{fold} - train Loss (MSE)\")\n",
        "    plt.plot(df['Epoch'], df[val_col], label=f\"{fold} - Validation Loss (MSE)\")\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"Model: {dic[i]}\\n Training Loss (MSE) and Validation Loss (MSE) vs epoches in {fold}\")\n",
        "    #plt.suptitle(f\"Model: {dic[1]}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "show_mse_statistics('trained_models/resnext101_32x8d_3_fold_adam_single_plastic_896x672_3/results_with_mse.csv', 3)\n",
        "show_mse_statistics('trained_models/resnext101_32x8d_3_fold_adam_single_plastic_cropped_1008x672_3/results_with_mse.csv', 4)\n",
        "show_mse_statistics('trained_models/resnext101_32x8d_3_fold_adam_single_plastic_4_3/results_with_mse.csv', 5)\n",
        "show_mse_statistics('trained_models/resnext101_32x8d_3_fold_adam_single_plastic_cropped_4_3/results_with_mse.csv', 6)\n",
        "show_mse_statistics('trained_models/resnext101_32x8d_3_fold_adam_single_plastic_5_4/results_with_mse.csv', 7)\n",
        "show_mse_statistics('trained_models/resnext101_32x8d_3_fold_adam_single_plastic_cropped_5_3/results_with_mse.csv', 8)\n",
        "show_mse_statistics('trained_models/resnext101_32x8d_3_fold_adam_single_plastic_data_aug_3/results_with_mse.csv', 9)\n",
        "show_mse_statistics('trained_models/resnext101_32x8d_3_fold_adam_single_plastic_cropped_data_aug_3/results_with_mse.csv', 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl-0PlY-R2Ui"
      },
      "source": [
        "## Test ANN with custom labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy5zlD32oqQL"
      },
      "outputs": [],
      "source": [
        "model_str = 'resnext101_32x8d'\n",
        "batch_size = 1\n",
        "labels_set = [0,1]\n",
        "combine_images = False\n",
        "image_type = 'plastic'\n",
        "\n",
        "# Initialize the model\n",
        "model = get_model(model_str)\n",
        "sd = torch.load('trained_models/resnext101_32x8d_3_fold_adam_single_plastic_cropped_3_3/fold_1_best_model.pt')\n",
        "save_dir = 'trained_models/resnext101_32x8d_3_fold_adam_single_plastic_cropped_3_3/testing_stats.xlsx'\n",
        "model.load_state_dict(sd)\n",
        "model.cuda().eval()  # Move model to GPU and set it to evaluation mode\n",
        "\n",
        "#def labeled_test(model, batch_size=1, labels_set=[], combine_images=True, image_type='both', save_path=\"test_results.xlsx\",  want_normalise=False):\n",
        "#resnext101_32x8d_3_fold_adam_single_plastic_2\n",
        "#resnext101_32x8d_3_fold_adam_single_plastic_cropped_2\n",
        "\n",
        "print('\\nTesting model for labels {}'.format(labels_set))\n",
        "labeled_test(model = model, batch_size = batch_size, want_normalise= True, labels_set= labels_set, combine_images=combine_images, image_type=image_type, save_path=save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uPCf3QXou13U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxSjNxQHhit0"
      },
      "source": [
        "# End of current code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y45r1_pyves"
      },
      "source": [
        "## Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0_CUdARyxIv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "del model\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.is_available())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}